{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kospi-2025/EVT/blob/main/EVT10_EVT_full.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PQrIF28BKOGb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import yfinance as yf\n",
        "import time\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# 1. GitHubÏóêÏÑú ticker_info.csv Î∂àÎü¨Ïò§Í∏∞\n",
        "ticker_info_url = \"https://raw.githubusercontent.com/kospi-2025/EVT/main/source_data/ticker_info.csv\"\n",
        "df = pd.read_csv(ticker_info_url)\n",
        "df['id'] = df['id'].astype(str).str.zfill(6)\n",
        "df['Yahoo_Ticker'] = df['id'] + \".KS\"\n",
        "\n",
        "# 2. ^KS200 ÏàòÎèô Ï∂îÍ∞Ä\n",
        "df_index = pd.DataFrame({\n",
        "    \"Yahoo_Ticker\": [\"^KS200\"],\n",
        "    \"name\": [\"KOSPI 200 Index\"],\n",
        "    \"sector\": [\"Index\"]\n",
        "})\n",
        "\n",
        "df_info = pd.concat([df, df_index], ignore_index=True)\n",
        "\n",
        "# 3. Îß§Ìïë ÎîïÏÖîÎÑàÎ¶¨ ÏÉùÏÑ±\n",
        "ticker_to_name = dict(zip(df_info[\"Yahoo_Ticker\"], df_info[\"name\"]))\n",
        "ticker_to_sector = dict(zip(df_info[\"Yahoo_Ticker\"], df_info[\"sector\"]))\n",
        "\n",
        "# 4. ÏÑπÌÑ∞ Î™©Î°ù ÎßåÎì§Í∏∞ (Í≥†Ïú†Í∞í)\n",
        "sectors = df_info[\"sector\"].dropna().unique()\n",
        "\n",
        "# 5. ÏÑπÌÑ∞Î≥Ñ ÌååÏùº Î∂àÎü¨Ïò§Í∏∞\n",
        "base_url = \"https://raw.githubusercontent.com/kospi-2025/EVT/main/source_data/\"\n",
        "sector_data = {}\n",
        "\n",
        "for sec in sectors:\n",
        "    file_name = sec + \".csv\"\n",
        "    url = f\"{base_url}{file_name}\"\n",
        "    try:\n",
        "        df_sector = pd.read_csv(url, header=[0, 1], index_col=0, parse_dates=True)\n",
        "        sector_data[sec] = df_sector\n",
        "        print(f\"‚úÖ Loaded {sec}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Failed to load {sec}: {e}\")\n",
        "\n",
        "#==================================\n",
        "\n",
        "temp = pd.concat(sector_data.values(), axis=1).sort_index(axis=1)\n",
        "\n",
        "tickers_to_drop = [\"000660.KS\", \"032640.KS\"]\n",
        "\n",
        "data = temp.loc[:, ~temp.columns.get_level_values(1).isin(tickers_to_drop)]\n",
        "logDD = -np.log(data[\"Low\"]/data[\"Close\"].shift(1)).where(lambda x: x < 0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50ju-Tr9G9UJ",
        "outputId": "cdb16f26-c969-4e64-d76e-adaf9633c6e5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Loaded Communication_Services\n",
            "‚úÖ Loaded Constructions\n",
            "‚úÖ Loaded Consumer_Discretionary\n",
            "‚úÖ Loaded Consumer_Staples\n",
            "‚úÖ Loaded Energy_Chemicals\n",
            "‚úÖ Loaded Financials\n",
            "‚úÖ Loaded Health_Care\n",
            "‚úÖ Loaded Heavy_Industries\n",
            "‚úÖ Loaded Industrials\n",
            "‚úÖ Loaded IT\n",
            "‚úÖ Loaded Steels_Materials\n",
            "‚úÖ Loaded Index\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ezpo8igbKQYQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16fce989-5c33-46c9-a803-d3d9b91b1a7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m46.5/46.5 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# ===========================================\n",
        "# üì¶ 1. Setup\n",
        "# ===========================================\n",
        "!pip install -q lmoments3\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.stats as stats\n",
        "from scipy.stats import genpareto\n",
        "import lmoments3 as lm\n",
        "from lmoments3 import distr\n",
        "\n",
        "# ===========================================\n",
        "# üìà 2. Helper Functions\n",
        "# ===========================================\n",
        "\n",
        "# 2.1 Basic Statistics\n",
        "def basic_statistics(exceedances):\n",
        "    return {\n",
        "        \"mean\": exceedances.mean(),\n",
        "        \"std\": exceedances.std(),\n",
        "        \"skewness\": stats.skew(exceedances),\n",
        "        \"kurtosis\": stats.kurtosis(exceedances, fisher=False)\n",
        "    }\n",
        "\n",
        "# 2.2 L-moment Estimation\n",
        "def fit_gpd_lmoment(exceedances):\n",
        "    try:\n",
        "        lmr = lm.lmom_ratios(exceedances, nmom=4)\n",
        "        params_lm = lm.distr.gpa.lmom_fit(exceedances)\n",
        "        l1, l2, tau3, tau4 = lmr\n",
        "        xi, beta = params_lm['c'], params_lm['scale']\n",
        "        return xi, beta, l1, l2, tau3, tau4\n",
        "    except Exception as e:\n",
        "        raise ValueError(f\"L-moment fitting failed: {e}\")\n",
        "\n",
        "# 2.3 MLE Estimation\n",
        "def fit_gpd_mle(exceedances):\n",
        "    params = genpareto.fit(exceedances, floc=0)\n",
        "    xi, loc, beta = params\n",
        "    return xi, beta\n",
        "\n",
        "# 2.4 Anderson-Darling statistic\n",
        "def ad_statistic(exceedances, xi, beta):\n",
        "    sorted_data = np.sort(exceedances)\n",
        "    F = genpareto.cdf(sorted_data, c=xi, loc=0, scale=beta)\n",
        "    F = np.clip(F, 1e-10, 1-1e-10)\n",
        "    n = len(sorted_data)\n",
        "    i = np.arange(1, n+1)\n",
        "    ad_stat = -n - np.mean((2*i - 1) * (np.log(F) + np.log(1 - F[::-1])))\n",
        "    return ad_stat\n",
        "\n",
        "# 2.5 Bootstrap AD p-value\n",
        "def bootstrap_ad_pvalue(exceedances, xi, beta, B=500):\n",
        "    real_ad = ad_statistic(exceedances, xi, beta)\n",
        "    n = len(exceedances)\n",
        "    bootstrap_ads = []\n",
        "    for _ in range(B):\n",
        "        synthetic = genpareto.rvs(c=xi, loc=0, scale=beta, size=n)\n",
        "        try:\n",
        "            ad_sim = ad_statistic(synthetic, xi, beta)\n",
        "            bootstrap_ads.append(ad_sim)\n",
        "        except:\n",
        "            continue\n",
        "    bootstrap_ads = np.array(bootstrap_ads)\n",
        "    p_value = np.mean(bootstrap_ads > real_ad)\n",
        "    return real_ad, p_value\n",
        "\n",
        "# 2.6 Log-likelihood for GPD\n",
        "def log_likelihood_gpd(data, xi, beta):\n",
        "    if beta <= 0:\n",
        "        return -np.inf\n",
        "    return np.sum(genpareto.logpdf(data, c=xi, loc=0, scale=beta))\n",
        "\n",
        "# 2.7 AIC and BIC\n",
        "def compute_aic_bic(logL, k, n):\n",
        "    aic = 2 * k - 2 * logL\n",
        "    bic = k * np.log(n) - 2 * logL\n",
        "    return aic, bic\n",
        "\n",
        "# ===========================================\n",
        "# ‚ö° 3. Master Analysis Function\n",
        "# ===========================================\n",
        "\n",
        "def analyze_one_ticker_upgraded(series, quantiles=[0.7,0.8,0.9,0.95,0.99], B=500, use_zero=True):\n",
        "    results = []\n",
        "    data = series.dropna().values\n",
        "    total_n = len(data)\n",
        "\n",
        "    thresholds = [np.quantile(data, q) for q in quantiles]\n",
        "    if use_zero:\n",
        "        thresholds = [0] + thresholds\n",
        "\n",
        "    for idx, threshold in enumerate(thresholds):\n",
        "        exceedances = data[data > threshold] - threshold\n",
        "        n_exceed = len(exceedances)\n",
        "\n",
        "        if n_exceed < 10:\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            # Basic Stats\n",
        "            basic = basic_statistics(exceedances)\n",
        "\n",
        "            # L-moment fitting\n",
        "            xi_L, beta_L, l1, l2, tau3, tau4 = fit_gpd_lmoment(exceedances)\n",
        "\n",
        "            # MLE fitting\n",
        "            xi_MLE, beta_MLE = fit_gpd_mle(exceedances)\n",
        "\n",
        "            # AD + bootstrap (L-moment fit)\n",
        "            ad_L, pval_L = bootstrap_ad_pvalue(exceedances, xi_L, beta_L, B=B)\n",
        "\n",
        "            # AD + bootstrap (MLE fit)\n",
        "            ad_MLE, pval_MLE = bootstrap_ad_pvalue(exceedances, xi_MLE, beta_MLE, B=B)\n",
        "\n",
        "            # Log-likelihoods and AIC/BIC\n",
        "            ll_L = log_likelihood_gpd(exceedances, xi_L, beta_L)\n",
        "            ll_MLE = log_likelihood_gpd(exceedances, xi_MLE, beta_MLE)\n",
        "            aic_L, bic_L = compute_aic_bic(ll_L, 2, n_exceed)\n",
        "            aic_MLE, bic_MLE = compute_aic_bic(ll_MLE, 2, n_exceed)\n",
        "\n",
        "            # Record everything\n",
        "            result = {\n",
        "                \"Total_n\": total_n,\n",
        "                \"Quantile\": 0 if idx == 0 else quantiles[idx-1],\n",
        "                \"Threshold\": threshold,\n",
        "                \"n_exceedances\": n_exceed,\n",
        "                \"mean\": basic[\"mean\"],\n",
        "                \"std\": basic[\"std\"],\n",
        "                \"skewness\": basic[\"skewness\"],\n",
        "                \"kurtosis\": basic[\"kurtosis\"],\n",
        "                \"L1\": l1,\n",
        "                \"L2\": l2,\n",
        "                \"tau3_Lskewness\": tau3,\n",
        "                \"tau4_Lkurtosis\": tau4,\n",
        "                \"GPD_shape_Lmoment\": xi_L,\n",
        "                \"GPD_scale_Lmoment\": beta_L,\n",
        "                \"GPD_shape_MLE\": xi_MLE,\n",
        "                \"GPD_scale_MLE\": beta_MLE,\n",
        "                \"AD_stat_Lmoment\": ad_L,\n",
        "                \"Bootstrap_pval_Lmoment\": pval_L,\n",
        "                \"AD_stat_MLE\": ad_MLE,\n",
        "                \"Bootstrap_pval_MLE\": pval_MLE,\n",
        "                \"LogL_Lmoment\": ll_L,\n",
        "                \"LogL_MLE\": ll_MLE,\n",
        "                \"AIC_Lmoment\": aic_L,\n",
        "                \"BIC_Lmoment\": bic_L,\n",
        "                \"AIC_MLE\": aic_MLE,\n",
        "                \"BIC_MLE\": bic_MLE\n",
        "            }\n",
        "            results.append(result)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Skipping threshold {threshold} due to error: {e}\")\n",
        "            continue\n",
        "\n",
        "    return pd.DataFrame(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "FTAl_DZbKTFn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2e542b9-1dc9-4a51-a14c-224eff626cf0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analyzing 000080.KS...\n",
            "Analyzing 000100.KS...\n",
            "Analyzing 000120.KS...\n",
            "Analyzing 000150.KS...\n",
            "Analyzing 000210.KS...\n",
            "Analyzing 000240.KS...\n",
            "Analyzing 000270.KS...\n",
            "Analyzing 000720.KS...\n",
            "Analyzing 000810.KS...\n",
            "Analyzing 000880.KS...\n",
            "Analyzing 001040.KS...\n",
            "Analyzing 001120.KS...\n",
            "Analyzing 001430.KS...\n",
            "Analyzing 001440.KS...\n",
            "Analyzing 001450.KS...\n",
            "Analyzing 001680.KS...\n",
            "Analyzing 001740.KS...\n",
            "Analyzing 001800.KS...\n",
            "Analyzing 002380.KS...\n",
            "Analyzing 002710.KS...\n",
            "Analyzing 002790.KS...\n",
            "Analyzing 002840.KS...\n",
            "Analyzing 003030.KS...\n",
            "Analyzing 003090.KS...\n",
            "Analyzing 003230.KS...\n",
            "Analyzing 003240.KS...\n",
            "Analyzing 003490.KS...\n",
            "Analyzing 003550.KS...\n",
            "Analyzing 003620.KS...\n",
            "Analyzing 003670.KS...\n",
            "Analyzing 004000.KS...\n",
            "Analyzing 004020.KS...\n",
            "Analyzing 004170.KS...\n",
            "Analyzing 004370.KS...\n",
            "Analyzing 004490.KS...\n",
            "Analyzing 004990.KS...\n",
            "Analyzing 005070.KS...\n",
            "Analyzing 005250.KS...\n",
            "Analyzing 005300.KS...\n",
            "Analyzing 005380.KS...\n",
            "Analyzing 005420.KS...\n",
            "Analyzing 005490.KS...\n",
            "Analyzing 005830.KS...\n",
            "Analyzing 005850.KS...\n",
            "Analyzing 005930.KS...\n",
            "Analyzing 005940.KS...\n",
            "Analyzing 006110.KS...\n",
            "Analyzing 006260.KS...\n",
            "Analyzing 006280.KS...\n",
            "Analyzing 006360.KS...\n",
            "Analyzing 006400.KS...\n",
            "Analyzing 006650.KS...\n",
            "Analyzing 006800.KS...\n",
            "Analyzing 007070.KS...\n",
            "Analyzing 007310.KS...\n",
            "Analyzing 008730.KS...\n",
            "Analyzing 008770.KS...\n",
            "Analyzing 008930.KS...\n",
            "Analyzing 009150.KS...\n",
            "Analyzing 009240.KS...\n",
            "Analyzing 009420.KS...\n",
            "Analyzing 009540.KS...\n",
            "Analyzing 009830.KS...\n",
            "Analyzing 009970.KS...\n",
            "Analyzing 010060.KS...\n",
            "Analyzing 010120.KS...\n",
            "Analyzing 010130.KS...\n",
            "Analyzing 010140.KS...\n",
            "Analyzing 010620.KS...\n",
            "Analyzing 010950.KS...\n",
            "Analyzing 011070.KS...\n",
            "Analyzing 011170.KS...\n",
            "Analyzing 011200.KS...\n",
            "Analyzing 011210.KS...\n",
            "Analyzing 011780.KS...\n",
            "Analyzing 011790.KS...\n",
            "Analyzing 012330.KS...\n",
            "Analyzing 012450.KS...\n",
            "Analyzing 012750.KS...\n",
            "Analyzing 014680.KS...\n",
            "Analyzing 014820.KS...\n",
            "Analyzing 015760.KS...\n",
            "Analyzing 016360.KS...\n",
            "Analyzing 017670.KS...\n",
            "Analyzing 017800.KS...\n",
            "Analyzing 018260.KS...\n",
            "Analyzing 018880.KS...\n",
            "Analyzing 021240.KS...\n",
            "Analyzing 022100.KS...\n",
            "Analyzing 023530.KS...\n",
            "Analyzing 024110.KS...\n",
            "Analyzing 026960.KS...\n",
            "Analyzing 028050.KS...\n",
            "Analyzing 028260.KS...\n",
            "Analyzing 028670.KS...\n",
            "Analyzing 029780.KS...\n",
            "Analyzing 030000.KS...\n",
            "Analyzing 030200.KS...\n",
            "Analyzing 032830.KS...\n",
            "Analyzing 033780.KS...\n",
            "Analyzing 034020.KS...\n",
            "Analyzing 034220.KS...\n",
            "Analyzing 034730.KS...\n",
            "Analyzing 035250.KS...\n",
            "Analyzing 035420.KS...\n",
            "Analyzing 035720.KS...\n",
            "Analyzing 036460.KS...\n",
            "Analyzing 036570.KS...\n",
            "Analyzing 039130.KS...\n",
            "Analyzing 039490.KS...\n",
            "Analyzing 042660.KS...\n",
            "Analyzing 042670.KS...\n",
            "Analyzing 042700.KS...\n",
            "Analyzing 047040.KS...\n",
            "Analyzing 047050.KS...\n",
            "Analyzing 047810.KS...\n",
            "Analyzing 051600.KS...\n",
            "Analyzing 051900.KS...\n",
            "Analyzing 051910.KS...\n",
            "Analyzing 052690.KS...\n",
            "Analyzing 055550.KS...\n",
            "Analyzing 064350.KS...\n",
            "Analyzing 066570.KS...\n",
            "Analyzing 066970.KS...\n",
            "Analyzing 068270.KS...\n",
            "Analyzing 069260.KS...\n",
            "Analyzing 069620.KS...\n",
            "Analyzing 069960.KS...\n",
            "Analyzing 071050.KS...\n",
            "Analyzing 073240.KS...\n",
            "Analyzing 078930.KS...\n",
            "Analyzing 079550.KS...\n",
            "Analyzing 081660.KS...\n",
            "Analyzing 086280.KS...\n",
            "Analyzing 086790.KS...\n",
            "Analyzing 088350.KS...\n",
            "Analyzing 090430.KS...\n",
            "Analyzing 093370.KS...\n",
            "Analyzing 096770.KS...\n",
            "Analyzing 097950.KS...\n",
            "Analyzing 103140.KS...\n",
            "Analyzing 105560.KS...\n",
            "Analyzing 105630.KS...\n",
            "Analyzing 111770.KS...\n",
            "Analyzing 112610.KS...\n",
            "Analyzing 114090.KS...\n",
            "Analyzing 120110.KS...\n",
            "Analyzing 128940.KS...\n",
            "Analyzing 137310.KS...\n",
            "Analyzing 138040.KS...\n",
            "Analyzing 138930.KS...\n",
            "Analyzing 139130.KS...\n",
            "Analyzing 139480.KS...\n",
            "Analyzing 145720.KS...\n",
            "Analyzing 161390.KS...\n",
            "Analyzing 161890.KS...\n",
            "Analyzing 175330.KS...\n",
            "Analyzing 178920.KS...\n",
            "Analyzing 180640.KS...\n",
            "Analyzing 185750.KS...\n",
            "Analyzing 192080.KS...\n",
            "Analyzing 192820.KS...\n",
            "Analyzing 204320.KS...\n",
            "Analyzing 207940.KS...\n",
            "Analyzing 241560.KS...\n",
            "Analyzing 251270.KS...\n",
            "Analyzing 259960.KS...\n",
            "Analyzing 267250.KS...\n",
            "Analyzing 267260.KS...\n",
            "Analyzing 271560.KS...\n",
            "Analyzing 271940.KS...\n",
            "Analyzing 272210.KS...\n",
            "Analyzing 278470.KS...\n",
            "Analyzing 280360.KS...\n",
            "Analyzing 282330.KS...\n",
            "Analyzing 285130.KS...\n",
            "Analyzing 298020.KS...\n",
            "Analyzing 298040.KS...\n",
            "Analyzing 298050.KS...\n",
            "Analyzing 300720.KS...\n",
            "Analyzing 302440.KS...\n",
            "Analyzing 316140.KS...\n",
            "Analyzing 323410.KS...\n",
            "Analyzing 326030.KS...\n",
            "Analyzing 329180.KS...\n",
            "Analyzing 336260.KS...\n",
            "Analyzing 352820.KS...\n",
            "Analyzing 361610.KS...\n",
            "Analyzing 373220.KS...\n",
            "Analyzing 375500.KS...\n",
            "Analyzing 377300.KS...\n",
            "Analyzing 383220.KS...\n",
            "Analyzing 402340.KS...\n",
            "Analyzing 450080.KS...\n",
            "Analyzing 454910.KS...\n",
            "Analyzing 456040.KS...\n",
            "Analyzing 457190.KS...\n",
            "Analyzing 489790.KS...\n",
            "Analyzing ^KS200...\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Assume you already have logDD loaded\n",
        "quantiles = [0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.99]\n",
        "B = 500\n",
        "\n",
        "ticker_results = []\n",
        "for ticker in logDD.columns:\n",
        "    print(f\"Analyzing {ticker}...\")\n",
        "    series = logDD[ticker]\n",
        "    df_ticker = analyze_one_ticker_upgraded(series, quantiles=quantiles, B=B, use_zero=True)\n",
        "    df_ticker[\"Ticker\"] = ticker\n",
        "    df_ticker[\"Sector\"] = ticker_to_sector.get(ticker, \"KOSPI200\")\n",
        "    df_ticker[\"Name\"] = ticker_to_name.get(ticker, ticker)\n",
        "    ticker_results.append(df_ticker)\n",
        "\n",
        "\n",
        "# Combine all into one big table\n",
        "final_table = pd.concat(ticker_results, ignore_index=True)\n",
        "\n",
        "# Save\n",
        "final_table.to_csv(\"evt_analysis_full.csv\", index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOJFyOVDQGZFZLZ3rriylgD",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}